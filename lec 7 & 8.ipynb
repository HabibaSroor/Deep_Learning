{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Autograde Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6229016948897019] [0.7417869892607294]\n"
     ]
    }
   ],
   "source": [
    "from random import Random\n",
    "from math import sqrt\n",
    "\n",
    "SEED = 5\n",
    "\n",
    "random_gen = Random(x = SEED)\n",
    "\n",
    "def gen_random (N=1):\n",
    "    data_x, data_y = [], []\n",
    "    for _ in range(N):\n",
    "        data_x.append(random_gen.uniform(a=0, b=1))\n",
    "    for _ in range(N):\n",
    "        data_y.append(random_gen.uniform(a=0, b=1))\n",
    "    return data_x, data_y\n",
    "data_x, data_y = gen_random()\n",
    "print(data_x, data_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5472122517293468\n"
     ]
    }
   ],
   "source": [
    "def loss(data_x, data_y, x_p = 0.3, y_p = 0.3):\n",
    "    N = len(data_x)\n",
    "    return (1/N) * sum([sqrt((x - x_p)**2 + (y - y_p)**2) for x,y in zip(data_x, data_y)])\n",
    "\n",
    "print(loss(data_x, data_y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how to calculate $\\frac{\\partial \\mathbb{L}}{\\partial x_p}$:\n",
    "<br><br>\n",
    "$$\n",
    "\\mathbb{L} = \\frac{1}{N} \\sum_{i=0}^{N-1}[(x_{i} - x_{p})^{2} + (y_{i} - y_{p})^{2}]^{\\frac{1}{2}}\n",
    "\\\\\n",
    "\\mathbb{L} = C \\sum_{i=0}^{N-1}{\\mathbb{L}(x_i, y_i)}\n",
    "\\\\\n",
    "where\\; C = \\frac{1}{N}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "\\\\\n",
    "\\frac{\\partial \\mathbb{L}}{\\partial x_p} = -((x_i - x_p)^2 + (y_i - y_p)^2)^\\frac{-1}{2} \\;\\; . \\;\\; (x_i - x_p) = \\frac{\\partial \\mathbb{L}(x_i, y_i)}{\\partial g_x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5900849147094943 -0.8073411877467226\n"
     ]
    }
   ],
   "source": [
    "def calc_grad(data_x, data_y, x_p=0.3, y_p=0.3):\n",
    "    grad_x, grad_y = 0. ,0.\n",
    "    for x_i, y_i in zip(data_x, data_y):\n",
    "        grad_x += ((((x_i-x_p)**2 + (y_i-y_p)**2)** -0.5) * (x_i-x_p))/len(data_x)\n",
    "        grad_y += (((x_i-x_p)**2 + (y_i-y_p)**2)** -0.5) * (y_i-y_p) /len(data_y)\n",
    "    return -grad_x, -grad_y\n",
    "x_grad, y_grad = calc_grad(data_x, data_y)\n",
    "print(x_grad, y_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5472, grad_fn=<MeanBackward0>)\n",
      "tensor([-0.5901, -0.8073])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "pnt = torch.tensor([0.3, 0.3])\n",
    "pnt.requires_grad = True\n",
    "pnt.retain_grad()\n",
    "data_torch = torch.tensor([data_x, data_y])\n",
    "data_torch = data_torch.t()\n",
    "\n",
    "loss_torch = torch.mean(torch.sqrt(((data_torch-pnt)**2) . sum(dim = 1)))\n",
    "\n",
    "loss_torch.backward()\n",
    "\n",
    "print(loss_torch)\n",
    "print(pnt.grad.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Autograd from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class comp_node:\n",
    "    def __init__(self, val, children = [], op = \"assign\"):\n",
    "        self.val = val\n",
    "        self.children = children\n",
    "        self.grad = 0\n",
    "        self.op = op\n",
    "        self.backword_prop = lambda : None\n",
    "\n",
    "\n",
    "    def __to_comp_node(self, obj):\n",
    "        if not isinstance(obj, comp_node):\n",
    "            return comp_node(val = obj)\n",
    "        else:\n",
    "            return obj\n",
    "    def __sub__(self, other):\n",
    "        \n",
    "        other = self.__to_comp_node(other)   \n",
    "        out = comp_node(val = (self.val - other.val),\n",
    "                        children=[self, other], op =\"subtraction\")\n",
    "        def _backword_pop():\n",
    "            self.grad += out.grad * 1\n",
    "            other.grad += out.grad * (-1)\n",
    "        out.backword_prop = _backword_pop      \n",
    "        return out\n",
    "    \n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        other = self.__to_comp_node(other)\n",
    "        out = comp_node(val = (self.val - other.val),\n",
    "                        children=[self, other],  op =\"subtraction\")\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, exponent):\n",
    "        if not isinstance(exponent, (int, float)):\n",
    "            raise ValueError(\"Unsupported type\")\n",
    "        out = comp_node(val = self.val ** exponent,\n",
    "                        children=[self], op = \"power\")\n",
    "        def _backword_prop():\n",
    "            self.grad +=out.grad * (exponent * self.val **(exponent - 1))\n",
    "        out.backword_prop = _backword_prop\n",
    "        return out\n",
    "\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.val == other.val\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = self.__to_comp_node(other)\n",
    "        out = comp_node(val = self.val + other.val, \n",
    "                        children= [self, other],  op =\"addition\")\n",
    "        def _backword_pop():\n",
    "            self.grad += out.grad * 1\n",
    "            other.grad += out.grad * 1\n",
    "        out.backword_prop = _backword_pop\n",
    "        return out\n",
    "    def __radd__(self, other):\n",
    "        other = self.__to_comp_node(other)\n",
    "        out = comp_node(val = (self.val + other.val),\n",
    "                        children=[self, other], op =\"addition\")\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = self.__to_comp_node(other)\n",
    "        out = comp_node(val = self.val * other.val, \n",
    "                        children=[self, other], op = \"mult\")\n",
    "        def _back_prop():\n",
    "            self.grad += out.grad * other.val\n",
    "            other.grad += out.grad * self.val\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        other = self.__to_comp_node(other)\n",
    "        return self * other\n",
    "    def __repr__(self):\n",
    "        return f\"op:{self.op} | val:{self.val:.4f} | children:{len(self.children)} | grad: {self.grad}\"\n",
    "\n",
    "\n",
    "    \n",
    "assert comp_node(val = 5). val == 5, \"assignment falied\"\n",
    "assert (comp_node(val = 5) - comp_node(val =3 )).val == 2\n",
    "assert(comp_node(val = 5) - 3 ).val ==2\n",
    "assert(5 - comp_node(val = 3).val ==2)\n",
    "assert (comp_node(val=5)**2).val == 25\n",
    "assert(comp_node(val=5)**2) == comp_node(val =25)\n",
    "assert (comp_node(val = 5) + comp_node(val =3 )).val == 8\n",
    "assert(comp_node(val = 5) + 3 ).val ==8\n",
    "assert(5 + comp_node(val = 3).val ==8)\n",
    "assert (comp_node(val=5)*2).val == 10\n",
    "assert(comp_node(val=5)*2) == comp_node(val =10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 op:power | val:0.5472 | children:1 | grad: 1\n",
      "1 op:addition | val:0.2994 | children:2 | grad: 0.9137222319490423\n",
      "2 op:power | val:0.1043 | children:1 | grad: 0.9137222319490423\n",
      "3 op:power | val:0.1952 | children:1 | grad: 0.9137222319490423\n",
      "4 op:subtraction | val:-0.3229 | children:2 | grad: -0.5900849147094943\n",
      "5 op:subtraction | val:-0.4418 | children:2 | grad: -0.8073411877467226\n",
      "6 op:assign | val:0.3000 | children:0 | grad: -0.5900849147094943\n",
      "7 op:assign | val:0.3000 | children:0 | grad: -0.8073411877467226\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_p, y_p = comp_node(val=0.3), comp_node(val=0.3)\n",
    "\n",
    "def loss_graph(x_p, y_p, data_x, data_y):\n",
    "    # print(data_x, data_y)\n",
    "    # print(f\"x_p{x_p.val} | y_p{y_p}\")\n",
    "    I_x, I_y = x_p - data_x, y_p - data_y\n",
    "    g_x, g_y = I_x**2, I_y **2\n",
    "    M = g_x + g_y\n",
    "    L = M **0.5\n",
    "    return L, [L, M, g_x,g_y, I_x, I_y, x_p, y_p]\n",
    "\n",
    "l, reverse_topo = loss_graph(x_p, y_p, data_x[0], data_y[0])\n",
    "reverse_topo[0].grad = 1\n",
    "for i, node in enumerate(reverse_topo):\n",
    "    node.backword_prop()\n",
    "    print(i, node)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
